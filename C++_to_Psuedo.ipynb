{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9d4IRdcVT8rI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import re\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGWr3UlbWD8-",
        "outputId": "b9c288dd-b3a8-4602-f0bd-d847df2b72aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SPOCDataset(Dataset):\n",
        "    def __init__(self, tsv_path, tokenizer_src, tokenizer_tgt, max_len=256):\n",
        "        self.data = pd.read_csv(tsv_path, sep='\\t', quoting=3)\n",
        "        self.tokenizer_src = tokenizer_src\n",
        "        self.tokenizer_tgt = tokenizer_tgt\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Assume 'text' column holds pseudocode and 'code' column holds C++ code.\n",
        "        src_text = self.data.iloc[idx]['code']\n",
        "        tgt_text = self.data.iloc[idx]['text']\n",
        "        src_tokens = self.tokenizer_src(str(src_text))[:self.max_len]\n",
        "        tgt_tokens = self.tokenizer_tgt(str(tgt_text))[:self.max_len]\n",
        "        return {'src': src_tokens, 'tgt': tgt_tokens}\n"
      ],
      "metadata": {
        "id": "OZ1LO34GVQcP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenizer(text):\n",
        "    # A simple whitespace and punctuation based tokenizer\n",
        "    tokens = re.findall(r\"[\\w]+|[^\\s\\w]\", text)\n",
        "    return tokens\n",
        "\n",
        "def build_vocab(tokenized_texts, min_freq=1):\n",
        "    vocab = {}\n",
        "    for tokens in tokenized_texts:\n",
        "        for token in tokens:\n",
        "            vocab[token] = vocab.get(token, 0) + 1\n",
        "    # Filter tokens by frequency\n",
        "    vocab = {token: freq for token, freq in vocab.items() if freq >= min_freq}\n",
        "\n",
        "    # Reserve indices for special tokens and assign them first.\n",
        "    specials = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
        "    vocab_with_specials = {}\n",
        "    for idx, sp in enumerate(specials):\n",
        "        vocab_with_specials[sp] = idx\n",
        "    # Now assign remaining tokens starting after the specials.\n",
        "    current_idx = len(specials)\n",
        "    for token in vocab:\n",
        "        if token not in vocab_with_specials:\n",
        "            vocab_with_specials[token] = current_idx\n",
        "            current_idx += 1\n",
        "    return vocab_with_specials"
      ],
      "metadata": {
        "id": "zTKXBk-EVRWX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "m4SwbMtNVXbp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert embed_size % num_heads == 0, \"Embedding size must be divisible by number of heads\"\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        # Learnable linear layers for queries, keys, and values.\n",
        "        self.fc_q = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_k = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_v = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        # Linear projections\n",
        "        Q = self.fc_q(query)  # (B, seq_len, embed_size)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        # Reshape for multiple heads: (B, num_heads, seq_len, head_dim)\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention = torch.softmax(scores, dim=-1)\n",
        "        out = torch.matmul(attention, V)  # (B, num_heads, seq_len, head_dim)\n",
        "\n",
        "        # Concat heads and pass through final linear layer.\n",
        "        out = out.transpose(1,2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "jCgZhCyfVgkD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, forward_expansion, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attention = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attention))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.norm2(x + self.dropout(forward))\n",
        "        return out\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, forward_expansion, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.encoder_attention = MultiHeadAttention(embed_size, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.norm3 = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
        "        # Self-attention with masking (for causal decoding)\n",
        "        self_attn = self.self_attention(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(self_attn))\n",
        "        # Encoder-decoder attention\n",
        "        enc_attn = self.encoder_attention(x, enc_out, enc_out, src_mask)\n",
        "        x = self.norm2(x + self.dropout(enc_attn))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.norm3(x + self.dropout(forward))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ZyZaIMTxVkbO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size=512, num_layers=6,\n",
        "                 num_heads=8, forward_expansion=4, dropout=0.1, max_len=100):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # Embedding layers for source and target languages.\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size, max_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(embed_size, num_heads, forward_expansion, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(embed_size, num_heads, forward_expansion, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        # src: (B, src_len)\n",
        "        # Create mask to ignore padding tokens (assume padding index is 0)\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        # tgt: (B, tgt_len)\n",
        "        B, T = tgt.shape\n",
        "        tgt_mask = torch.tril(torch.ones((T, T), device=tgt.device)).expand(B, 1, T, T)\n",
        "        return tgt_mask\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        x = self.dropout(self.positional_encoding(self.src_embedding(src)))\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x, src_mask)\n",
        "        return x\n",
        "\n",
        "    def decode(self, tgt, enc_out, src_mask, tgt_mask):\n",
        "        x = self.dropout(self.positional_encoding(self.tgt_embedding(tgt)))\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, enc_out, src_mask, tgt_mask)\n",
        "        return x\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        enc_out = self.encode(src, src_mask)\n",
        "        dec_out = self.decode(tgt, enc_out, src_mask, tgt_mask)\n",
        "        out = self.fc_out(dec_out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "BLMw0W1nVnkR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(batch, src_vocab, tgt_vocab):\n",
        "    \"\"\"\n",
        "    Prepares a batch by converting token lists into padded tensors.\n",
        "\n",
        "    Args:\n",
        "        batch (list): A list of dictionaries with keys 'src' and 'tgt'.\n",
        "        src_vocab (dict): Source vocabulary mapping tokens to indices.\n",
        "        tgt_vocab (dict): Target vocabulary mapping tokens to indices.\n",
        "\n",
        "    Returns:\n",
        "        src_tensor (torch.Tensor): Padded tensor of source sequences.\n",
        "        tgt_tensor (torch.Tensor): Padded tensor of target sequences.\n",
        "    \"\"\"\n",
        "    # Extract token lists for source and target sequences\n",
        "    src_batch = [sample['src'] for sample in batch]\n",
        "    tgt_batch = [sample['tgt'] for sample in batch]\n",
        "\n",
        "    # Convert tokens to indices using the provided vocabularies.\n",
        "    # Use '<unk>' if a token is not found.\n",
        "    src_indices = [\n",
        "        [src_vocab.get(token, src_vocab.get('<unk>')) for token in tokens]\n",
        "        for tokens in src_batch\n",
        "    ]\n",
        "    tgt_indices = [\n",
        "        [tgt_vocab.get(token, tgt_vocab.get('<unk>')) for token in tokens]\n",
        "        for tokens in tgt_batch\n",
        "    ]\n",
        "\n",
        "    # Determine maximum sequence lengths in the batch\n",
        "    max_src_len = max(len(seq) for seq in src_indices)\n",
        "    max_tgt_len = max(len(seq) for seq in tgt_indices)\n",
        "\n",
        "    # Get pad token indices; default to 0 if not explicitly defined\n",
        "    src_pad_idx = src_vocab.get('<pad>', 0)\n",
        "    tgt_pad_idx = tgt_vocab.get('<pad>', 0)\n",
        "\n",
        "    # Pad each sequence to the maximum length in the batch\n",
        "    padded_src = [\n",
        "        seq + [src_pad_idx] * (max_src_len - len(seq)) for seq in src_indices\n",
        "    ]\n",
        "    padded_tgt = [\n",
        "        seq + [tgt_pad_idx] * (max_tgt_len - len(seq)) for seq in tgt_indices\n",
        "    ]\n",
        "\n",
        "    # Convert the padded lists to PyTorch tensors\n",
        "    src_tensor = torch.tensor(padded_src, dtype=torch.long)\n",
        "    tgt_tensor = torch.tensor(padded_tgt, dtype=torch.long)\n",
        "\n",
        "    return src_tensor, tgt_tensor\n"
      ],
      "metadata": {
        "id": "m8oV54xjdmpY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/spoc/train/spoc-train.tsv', sep='\\t', quoting=3)\n",
        "\n",
        "# Convert the 'text' and 'code' columns to strings and tokenize them\n",
        "src_texts = train_data['text'].astype(str).tolist()\n",
        "tgt_texts = train_data['code'].astype(str).tolist()\n",
        "\n",
        "# Tokenize the texts using your simple_tokenizer\n",
        "tokenized_src_texts = [simple_tokenizer(text) for text in src_texts]\n",
        "tokenized_tgt_texts = [simple_tokenizer(code) for code in tgt_texts]\n",
        "\n",
        "# Build vocabularies using your build_vocab function\n",
        "src_vocab = build_vocab(tokenized_src_texts)\n",
        "tgt_vocab = build_vocab(tokenized_tgt_texts)\n",
        "\n",
        "# Optionally, print out vocabulary sizes\n",
        "print(f\"Source Vocabulary Size: {len(src_vocab)}\")\n",
        "print(f\"Target Vocabulary Size: {len(tgt_vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOFo0RkUbvge",
        "outputId": "a0190208-5010-4cdc-88e7-b63628bed533"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source Vocabulary Size: 7427\n",
            "Target Vocabulary Size: 6153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/C++_to_Pseudo/src_vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(src_vocab, f)\n",
        "\n",
        "with open('/content/drive/MyDrive/C++_to_Pseudo/tgt_vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(tgt_vocab, f)\n",
        "\n",
        "print(\"Vocabularies saved to disk.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIPJmjfJ-OF2",
        "outputId": "d9b7e142-f031-499b-e125-d91ae4e7f1fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabularies saved to disk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_VOCAB_SIZE = 7427   # update based on built vocabulary\n",
        "TGT_VOCAB_SIZE = 6153   # update based on built vocabulary\n",
        "EMBED_SIZE = 128\n",
        "NUM_LAYERS = 2\n",
        "NUM_HEADS = 2\n",
        "FORWARD_EXPANSION = 4\n",
        "DROPOUT = 0.1\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 3e-4\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Instantiate model\n",
        "model = Transformer(SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, EMBED_SIZE,\n",
        "                    num_layers=NUM_LAYERS, num_heads=NUM_HEADS,\n",
        "                    forward_expansion=FORWARD_EXPANSION, dropout=DROPOUT)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # assuming <pad> token index is 0\n",
        "\n",
        "def train():\n",
        "    # Load and tokenize training data.\n",
        "    train_dataset = SPOCDataset('/content/drive/MyDrive/spoc/train/spoc-train.tsv', simple_tokenizer, simple_tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda x: x)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0\n",
        "        # Wrap train_loader with tqdm to show progress\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False)\n",
        "        for batch in progress_bar:\n",
        "            # Convert token lists to tensors and pad sequences appropriately.\n",
        "            src_tensor, tgt_tensor = prepare_batch(batch, src_vocab, tgt_vocab)\n",
        "            optimizer.zero_grad()\n",
        "            # Shift target tokens for teacher forcing.\n",
        "            output = model(src_tensor, tgt_tensor[:, :-1])\n",
        "            # Reshape output and target for loss computation.\n",
        "            output = output.reshape(-1, TGT_VOCAB_SIZE)\n",
        "            tgt_out = tgt_tensor[:, 1:].reshape(-1)\n",
        "            loss = criterion(output, tgt_out)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix(loss=loss.item())\n",
        "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm13tthJVrKx",
        "outputId": "0c327c4d-1e4e-48c3-bd91-a227fc1f8f4e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3, Loss: 1.5044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3, Loss: 0.8707\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                          "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3, Loss: 0.7220\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/content/drive/MyDrive/C++_to_Pseudo/model.pth')"
      ],
      "metadata": {
        "id": "0FU5oQXqYtF9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    # Optionally, calculate BLEU scores or use a C++ compiler to check syntactic correctness.\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            src_tensor, tgt_tensor = prepare_batch(batch,src_vocab,tgt_vocab)  # same helper as in training\n",
        "            output = model(src_tensor, tgt_tensor[:, :-1])\n",
        "            # Compute loss and any additional metrics here.\n",
        "    # Return or print evaluation metrics.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your trained model and evaluation dataset.\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "2jFG7bQZYFBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_code(model, pseudocode, src_vocab, tgt_vocab, max_length=100, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generates C++ code from input pseudocode using a trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained Transformer model.\n",
        "        pseudocode (str): The input pseudocode string.\n",
        "        src_vocab (dict): Source vocabulary mapping tokens to indices.\n",
        "        tgt_vocab (dict): Target vocabulary mapping tokens to indices.\n",
        "        max_length (int): Maximum number of tokens to generate.\n",
        "        device (str): Device to run the model on ('cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        generated_code (str): The generated C++ code.\n",
        "    \"\"\"\n",
        "    # Ensure the model is on the correct device.\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the pseudocode and convert tokens to indices using the source vocabulary.\n",
        "        src_tokens = simple_tokenizer(str(pseudocode))\n",
        "        src_indices = [src_vocab.get(token, src_vocab.get('<unk>')) for token in src_tokens]\n",
        "\n",
        "        # Convert to tensor with shape [1, src_seq_len] and send to device.\n",
        "        src_tensor = torch.tensor(src_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "        src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "        # Pass through the encoder.\n",
        "        encoder_output = model.encode(src_tensor, src_mask)\n",
        "\n",
        "        # Initialize target sequence with the start-of-sequence token.\n",
        "        sos_token = tgt_vocab.get('<sos>')\n",
        "        eos_token = tgt_vocab.get('<eos>')\n",
        "        generated_tokens = [sos_token]\n",
        "\n",
        "        # Greedy decoding loop.\n",
        "        for _ in range(max_length):\n",
        "            # Create target tensor from generated tokens so far.\n",
        "            tgt_tensor = torch.tensor(generated_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "            tgt_mask = model.make_tgt_mask(tgt_tensor)\n",
        "\n",
        "            # Decode using encoder output and current target sequence.\n",
        "            decoder_output = model.decode(tgt_tensor, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "            # Pass through the final linear layer to get logits over the target vocabulary.\n",
        "            logits = model.fc_out(decoder_output)  # shape: [1, seq_len, tgt_vocab_size]\n",
        "            # Focus on the logits of the last token.\n",
        "            next_token_logits = logits[:, -1, :]  # shape: [1, tgt_vocab_size]\n",
        "            # Greedy decoding: select the token with the highest logit.\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
        "\n",
        "            generated_tokens.append(next_token)\n",
        "\n",
        "            # Stop if the end-of-sequence token is generated.\n",
        "            if next_token == eos_token:\n",
        "                break\n",
        "\n",
        "        # Build a reverse mapping from indices to tokens for the target vocabulary.\n",
        "        rev_tgt_vocab = {idx: token for token, idx in tgt_vocab.items()}\n",
        "        generated_token_list = [rev_tgt_vocab.get(idx, '<unk>') for idx in generated_tokens]\n",
        "\n",
        "        # Remove the start token and tokens after the end token.\n",
        "        if generated_token_list[0] == '<sos>':\n",
        "            generated_token_list = generated_token_list[1:]\n",
        "        if '<eos>' in generated_token_list:\n",
        "            eos_index = generated_token_list.index('<eos>')\n",
        "            generated_token_list = generated_token_list[:eos_index]\n",
        "\n",
        "        # Join tokens into a string (adjust spacing/formatting as needed).\n",
        "        generated_code = ' '.join(generated_token_list)\n",
        "\n",
        "    return generated_code\n",
        "\n",
        "\n",
        "code = \"int a=2;int b=3;\"\n",
        "code_output = generate_code(model, code, src_vocab, tgt_vocab, max_length=100, device='cuda')\n",
        "print(code_output)\n"
      ],
      "metadata": {
        "id": "6AwdiUxaiY_I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81294dc-5b6a-40a3-a33b-7c9f29ead32c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= integer <unk> a = 2 , b = 3 as integer = 3 ; integer = 3 to 3 <unk> integer <unk> integer a = 2 to 3 as integer <unk> integer <unk> integer = 2 = 2 to 3 = 3 as integer <unk> integer <unk> integer a = 3 = 3 = 3 = 3 = 3 = 3 = 3 as integer <unk> integer <unk> integer <unk> integer <unk> integer array b = 2 to 3 to 3 = 2 to 3 as integer <unk> integer <unk> integer = 2 to 3 = 2 to 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_path, src_vocab_size, tgt_vocab_size, embed_size=128, num_layers=2,\n",
        "               num_heads=2, forward_expansion=4, dropout=0.1, max_len=100, device='cpu'):\n",
        "    \"\"\"\n",
        "    Loads the Transformer model from a saved state dictionary.\n",
        "    \"\"\"\n",
        "    model = Transformer(src_vocab_size, tgt_vocab_size, embed_size, num_layers,\n",
        "                        num_heads, forward_expansion, dropout, max_len)\n",
        "    state_dict = torch.load(model_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def load_model_and_generate(model_path, pseudocode, src_vocab, tgt_vocab, device='cuda'):\n",
        "    \"\"\"\n",
        "    Loads the model from the specified path and generates C++ code from pseudocode.\n",
        "    \"\"\"\n",
        "    # Determine vocabulary sizes.\n",
        "    src_vocab_size = len(src_vocab)\n",
        "    tgt_vocab_size = len(tgt_vocab)\n",
        "    # Load the model.\n",
        "    model = load_model(model_path, src_vocab_size, tgt_vocab_size, device=device)\n",
        "    # Generate code.\n",
        "    generated_code = generate_code(model, pseudocode, src_vocab, tgt_vocab, max_length=100, device=device)\n",
        "    return generated_code"
      ],
      "metadata": {
        "id": "GXmjP4IV9EVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/Pesudo_to_C++/model'\n",
        "pseudocode = \"n , nn, ans = integers with ans = 0\"\n",
        "\n",
        "# Generate code using the loaded model.\n",
        "code_output = load_model_and_generate(model_path, pseudocode, src_vocab, tgt_vocab, device='cuda')\n",
        "print(code_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsC72r9F9RS2",
        "outputId": "c5b5600a-691d-4061-c176-22979009e961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-1fcf024c926a>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "= 0 , nn ; ans = 0 ; ans = 0 ; + + ) { ans = 0 ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; } ; }\n"
          ]
        }
      ]
    }
  ]
}